import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn import preprocessing
%matplotlib inline


""" About Dataset
Imagine a telecommunications provider has segmented its customer base by service usage patterns, categorizing the customers into four groups. If demographic data can be used to predict group membership, the company can customize offers for individual prospective customers. It is a classification problem. That is, given the dataset, with predefined labels, we need to build a model to be used to predict class of a new or unknown case.

The example focuses on using demographic data, such as region, age, and marital, to predict usage patterns.

The target field, called custcat, has four possible values that correspond to the four customer groups, as follows: 1- Basic Service 2- E-Service 3- Plus Service 4- Total Service

Our objective is to build a classifier, to predict the class of unknown cases. We will use a specific type of classification called K nearest neighbour.

"""

#read dataset with pandas
df = pd.read_csv('dataset.csv')
df.head() #short look to data


#see how many value in each category
df['custcat'].value_counts()
#281 Plus Service, 266 Basic-service, 236 Total Service, and 217 E-Service customers

#let inspect histogram of income column
df.hist(column='income', bins=50)

#define feature names
df.columns 

#we have to convert pandas dataframe to numpy array
X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float)
X[0:5]

#label
y = df['custcat'].values
y[0:5]

#data standardization
X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]

#split train and test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)
"""
Train set: (800, 11) (800,)
Test set: (200, 11) (200,)
"""

#define knn
from sklearn.neighbors import KNeighborsClassifier
k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
neigh

#make a prediction using x_test
yhat = neigh.predict(X_test)
yhat[0:5]

#check the accuracy using jaccard_score 
from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))
"""
Train set Accuracy:  0.5475
Test set Accuracy:  0.32
"""


#Lets check if we change the k to 6

k=6
kn = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
kn

yhat = kn.predict(X_test)
yhat[0:5]

print("Train set Accuracy: ", metrics.accuracy_score(y_train, kn.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))

"""
the accuracy has a little bit changing
Train set Accuracy:  0.51625
Test set Accuracy:  0.31
"""

# How can we define the best K?

Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))

for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)

    
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc

# If we draw the plot of K, we can see the best K choose

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color="green")
plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neighbors (K)')
plt.tight_layout()
plt.show()

#Moreover if we want to write the best K, here the code
print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
